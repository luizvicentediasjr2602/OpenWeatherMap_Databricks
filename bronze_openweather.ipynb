{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5321b41c-5a10-4864-b5bd-dfc3f0e74fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.0 Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "929fffef-7d99-482b-80c7-484829106606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q python-dotenv\n",
    "\n",
    "import os, json, time, datetime as dt\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import requests\n",
    "from dotenv import dotenv_values\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "from pyspark.sql import Row, functions as F\n",
    "\n",
    "from datetime import timezone\n",
    "\n",
    "import types as T\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F, types as T\n",
    "import uuid, datetime as dt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ff3c36b-d39a-4654-a06e-da8d35e82827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c645ac07-9ba1-403c-b6a9-24bcfbe5f962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.0 Conexão com API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd59cb9-acd8-4691-9511-abfb4ad5e4c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user = spark.sql(\"SELECT current_user()\").first()[0]\n",
    "env_path = f\"/Workspace/Users/{user}/config/config.env\"\n",
    "\n",
    "cfg = dotenv_values(env_path) or {}\n",
    "API_KEY = cfg.get(\"OWM_API_KEY\") or os.getenv(\"OWM_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(f\"OWM_API_KEY não encontrada em {env_path}. Verifique o arquivo .env.\")\n",
    "\n",
    "print(\"Conexão com API OpenWeather ESTABELECIDA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d783c2-395d-4a95-9113-aaeec8986b98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.1 Cidades alvos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17373f62-69c2-49eb-af7a-6a08c5738e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BASE_URL = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "\n",
    "CITIES = [\n",
    "    \"Rio Branco,BR\",\n",
    "    \"Maceio,BR\",\n",
    "    \"Macapa,BR\",\n",
    "    \"Manaus,BR\",\n",
    "    \"Salvador,BR\",\n",
    "    \"Fortaleza,BR\",\n",
    "    \"Brasilia,BR\",\n",
    "    \"Vitoria,BR\",\n",
    "    \"Goiania,BR\",\n",
    "    \"Sao Luis,BR\",\n",
    "    \"Cuiaba,BR\",\n",
    "    \"Campo Grande,BR\",\n",
    "    \"Belo Horizonte,BR\",\n",
    "    \"Belem,BR\",\n",
    "    \"Joao Pessoa,BR\",\n",
    "    \"Curitiba,BR\",\n",
    "    \"Recife,BR\",\n",
    "    \"Teresina,BR\",\n",
    "    \"Rio de Janeiro,BR\",\n",
    "    \"Natal,BR\",\n",
    "    \"Porto Alegre,BR\",\n",
    "    \"Porto Velho,BR\",\n",
    "    \"Boa Vista,BR\",\n",
    "    \"Florianopolis,BR\",\n",
    "    \"Sao Paulo,BR\",\n",
    "    \"Aracaju,BR\",\n",
    "    \"Palmas,BR\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14eb05a8-b51f-4ec5-94d2-7ec00a1a033f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3.2 Ingestão de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2dbb328-9fbd-41cf-96e5-a8898676703c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agora_utc   = dt.datetime.now(timezone.utc).replace(tzinfo=None, microsecond=0)\n",
    "agora_local = dt.datetime.now(ZoneInfo(\"America/Sao_Paulo\")).replace(microsecond=0)\n",
    "\n",
    "ESPERA_BASE_S = 1.0             # pausa entre chamadas (anti rate-limit)\n",
    "TIMEOUT       = (3, 10)         # connect, read\n",
    "\n",
    "registros = []\n",
    "\n",
    "with requests.Session() as sessao:\n",
    "    sessao.headers.update({\"User-Agent\": \"openweather-ingest/1.0\"})\n",
    "    for cidade in CITIES:\n",
    "        try:\n",
    "            resp = sessao.get(\n",
    "                BASE_URL,\n",
    "                params={\"q\": cidade, \"appid\": API_KEY, \"units\": \"metric\", \"lang\": \"pt_br\"},\n",
    "                timeout=TIMEOUT,\n",
    "            )\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            data = resp.json()\n",
    "            obs_ts_utc = (\n",
    "                dt.datetime.utcfromtimestamp(int(data[\"dt\"]))\n",
    "                if isinstance(data.get(\"dt\"), (int, float)) else None\n",
    "            )\n",
    "\n",
    "            registros.append({\n",
    "                \"city_id\": str(data.get(\"id\") or \"\"),\n",
    "                \"json_line\": json.dumps(data, ensure_ascii=False),\n",
    "                \"obs_ts_utc\": obs_ts_utc,\n",
    "                \"ingestion_ts_utc\": agora_utc,\n",
    "                \"ingestion_date\": agora_local.date(),\n",
    "            })\n",
    "            print(f\"OK: {cidade}\")\n",
    "\n",
    "        except requests.HTTPError as e:\n",
    "            codigo = getattr(e.response, \"status_code\", None)\n",
    "            print(f\"[HTTP {codigo}] {cidade} -> {e}\")\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"[REQ ERROR] {cidade} -> {e}\")\n",
    "\n",
    "        \n",
    "        time.sleep(ESPERA_BASE_S)\n",
    "\n",
    "if not registros:\n",
    "    raise RuntimeError(\"Nenhum registro coletado.\")\n",
    "\n",
    "df_bronze = spark.createDataFrame(registros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9832772a-fcbc-4edf-8801-c391eceb9af1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.3 Auditoria de dados \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7230648-7261-456c-9e4b-ac6d313a2d89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tabela_bronze = \"workspace.weather.bronze_openweather_raw\"\n",
    "tabela_logs   = \"workspace.weather.ingestion_logs\"\n",
    "\n",
    "# 1) Lote que irá para a tabela bronze, sem nulos e sem duplicatas \n",
    "\n",
    "colunas_bronze = [\"city_id\",\"json_line\",\"obs_ts_utc\",\"ingestion_ts_utc\",\"ingestion_date\"]\n",
    "df_lote = (df_bronze\n",
    "           .select(*colunas_bronze)\n",
    "           .filter(F.col(\"obs_ts_utc\").isNotNull())\n",
    "           .dropDuplicates([\"city_id\", \"obs_ts_utc\"])\n",
    ")\n",
    "\n",
    "# 2) Variáveis que serão gravadas na tabela de logs\n",
    "\n",
    "id_execucao      = str(uuid.uuid4())\n",
    "inicio_utc       = dt.datetime.now(timezone.utc).replace(microsecond=0)\n",
    "\n",
    "registros_tentados  = df_bronze.count()\n",
    "registros_no_lote   = df_lote.count()\n",
    "cidades_alvo        = len(CITIES)\n",
    "\n",
    "chaves_destino            = spark.table(tabela_bronze).select(\"city_id\",\"obs_ts_utc\").dropDuplicates()\n",
    "ja_existentes             = (df_lote.select(\"city_id\",\"obs_ts_utc\")\n",
    "                                .join(chaves_destino, [\"city_id\",\"obs_ts_utc\"], \"inner\")\n",
    "                                .count())\n",
    "registros_ja_existentes   = ja_existentes\n",
    "registros_inseridos       = registros_no_lote - registros_ja_existentes\n",
    "\n",
    "# 3) Escreve na tabela bronze\n",
    "status_execucao, mensagem_erro = \"SUCCESS\", None\n",
    "try:\n",
    "    (DeltaTable.forName(spark, tabela_bronze)\n",
    "       .alias(\"t\")\n",
    "       .merge(df_lote.alias(\"s\"), \"t.city_id = s.city_id AND t.obs_ts_utc = s.obs_ts_utc\")\n",
    "       .whenNotMatchedInsertAll()\n",
    "       .execute())\n",
    "except Exception as e:\n",
    "    status_execucao = \"FAIL\"\n",
    "    mensagem_erro   = str(e)\n",
    "\n",
    "fim_utc          = dt.datetime.now(timezone.utc).replace(microsecond=0)\n",
    "duracao_segundos = (fim_utc - inicio_utc).total_seconds()\n",
    "data_ingestao    = df_lote.agg(F.max(\"ingestion_date\")).first()[0]\n",
    "\n",
    "# 4) Escreve na tabela de logs  \n",
    "schema_log = T.StructType([\n",
    "    T.StructField(\"id_execucao\",             T.StringType()),\n",
    "    T.StructField(\"data_inicio\",             T.TimestampType()),\n",
    "    T.StructField(\"data_fim\",                T.TimestampType()),\n",
    "    T.StructField(\"duracao_segundos\",        T.DoubleType()),\n",
    "    T.StructField(\"data_ingestao\",           T.DateType()),\n",
    "    T.StructField(\"cidades_alvo\",            T.IntegerType()),\n",
    "    T.StructField(\"registros_tentados\",      T.IntegerType()),\n",
    "    T.StructField(\"registros_no_lote\",       T.IntegerType()),\n",
    "    T.StructField(\"registros_ja_existentes\", T.IntegerType()),\n",
    "    T.StructField(\"registros_inseridos\",     T.IntegerType()),\n",
    "    T.StructField(\"status\",                  T.StringType()),\n",
    "    T.StructField(\"mensagem_erro\",           T.StringType()),\n",
    "])\n",
    "\n",
    "linha_log = [(\n",
    "    id_execucao, inicio_utc, fim_utc, float(duracao_segundos),\n",
    "    data_ingestao, int(cidades_alvo),\n",
    "    int(registros_tentados), int(registros_no_lote),\n",
    "    int(registros_ja_existentes), int(registros_inseridos),\n",
    "    status_execucao, mensagem_erro\n",
    ")]\n",
    "\n",
    "spark.createDataFrame(linha_log, schema=schema_log) \\\n",
    "     .write.format(\"delta\").mode(\"append\").saveAsTable(\"workspace.weather.ingestion_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd567a70-93b0-4a9b-aeaf-fa68926775cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  spark.table(tabela_bronze)\n",
    "       .orderBy(F.col(\"ingestion_ts_utc\").desc())\n",
    "       .limit(5)\n",
    ")\n",
    "\n",
    "display(\n",
    "  spark.table(tabela_logs)\n",
    "       .orderBy(F.col(\"data_inicio\").desc())\n",
    "       .limit(1)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6590701826869146,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_openweather",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
